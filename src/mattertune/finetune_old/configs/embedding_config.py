from typing import cast
from collections.abc import Iterable
from mattertune.finetune.configs import OptimizerConfig, _OptimizerParamGroupConfig
from pydantic import BaseModel
import torch.nn as nn
from logging import getLogger
import copy


log = getLogger(__name__)

class EmbeddingConfig(BaseModel):
    num_elements: int
    embedding_size: int


def _create_dict_from_config(
    config: OptimizerConfig,
    params: Iterable[nn.Parameter],
    name: str | None = None,
):
    # This is a hack to get type hints for the kwargs
    # of the module while actually returning a dict.
    from torch.optim import AdamW

    AdamWKwargs = AdamW

    if config.lr <= 0:
        raise ValueError(f"Learning rate must be positive, got {config.lr}")

    kwargs = cast(
        dict,
        AdamWKwargs(
            params=params,
            lr=config.lr,
            amsgrad=config.amsgrad,
            weight_decay=config.weight_decay,
            betas=config.betas,
            eps=config.eps,
        ),
    )
    if name is not None:
        kwargs["name"] = name
    return _OptimizerParamGroupConfig(AdamW, param_group_kwargs=kwargs)


def optimizer_from_config(
    param_groups: list[tuple[OptimizerConfig, Iterable[nn.Parameter]]]
    | list[tuple[OptimizerConfig, Iterable[nn.Parameter], str | None]],
    *,
    base: "OptimizerConfig | None" = None,
):
    configs = [
        _create_dict_from_config(
            param_group[0],
            param_group[1],
            name=param_group[2] if len(param_group) == 3 else None,
        )
        for param_group in param_groups
    ]
    optimizer_cls_list = [c.cls for c in configs]
    assert len(set(optimizer_cls_list)) == 1, "All optimizers must be of the same type"
    optimizer_cls = optimizer_cls_list[0]

    optimizer_kwargs_list = [c.optimizer_kwargs for c in configs]
    assert (
        len(set(map(str, optimizer_kwargs_list))) == 1
    ), "All optimizers must have the same kwargs"
    optimizer_kwargs = optimizer_kwargs_list[0]

    base_kwargs = {}
    if base is not None:
        base_config = _create_dict_from_config(base, [])
        assert (
            base_config.cls == optimizer_cls
        ), "Base optimizer must be of the same type"
        _ = base_config.param_group_kwargs.pop("params", None)
        base_kwargs.update(base_config.param_group_kwargs)

    param_groups_configs = [c.param_group_kwargs for c in configs]
    optimizer = optimizer_cls(
        params=param_groups_configs,
        **optimizer_kwargs,
        **base_kwargs,
    )
    # detailed log about the optimizer configuration
    param_groups_logs: list[str] = []
    for i, c in enumerate(param_groups_configs):
        c = copy.deepcopy(c)
        params = c.pop("params", None)
        n_params = len(params) if params is not None else 0
        total_param_size = sum(p.numel() for p in params) if params is not None else 0
        param_groups_logs.append(
            f"Param group {i}:\n"
            f"    Params: {n_params}\n"
            f"    Total param size: {total_param_size}\n"
            f"    Other kwargs: {c}"
        )
    param_groups_log = "\n".join(param_groups_logs)
    log.critical(
        f"Optimizer: {optimizer_cls.__name__}\n"
        f"Optimizer kwargs: {optimizer_kwargs}\n"
        f"Base kwargs: {base_kwargs}\n"
        f"Param groups: {param_groups_log}"
    )
    return optimizer
